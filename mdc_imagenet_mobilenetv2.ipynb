{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mmcv\n",
      "  Using cached mmcv-1.1.4.tar.gz (239 kB)\n",
      "Requirement already satisfied: dotmap in /home/hongky/.conda/envs/mdc/lib/python3.6/site-packages (1.3.17)\n",
      "Collecting addict\n",
      "  Using cached addict-2.3.0-py3-none-any.whl (3.7 kB)\n",
      "Requirement already satisfied: numpy in /home/hongky/.conda/envs/mdc/lib/python3.6/site-packages (from mmcv) (1.19.1)\n",
      "Processing /home/hongky/.cache/pip/wheels/e5/9d/ad/2ee53cf262cba1ffd8afe1487eef788ea3f260b7e6232a80fc/PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl\n",
      "Collecting yapf\n",
      "  Using cached yapf-0.30.0-py2.py3-none-any.whl (190 kB)\n",
      "Requirement already satisfied: opencv-python>=3 in /home/hongky/.conda/envs/mdc/lib/python3.6/site-packages (from mmcv) (4.3.0.36)\n",
      "Building wheels for collected packages: mmcv\n",
      "  Building wheel for mmcv (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mmcv: filename=mmcv-1.1.4-cp36-cp36m-linux_x86_64.whl size=401669 sha256=341883e4f2ebaccc5fcb9aa7d39d8ee1104bc578df5a451c4022ed92ba9e19f0\n",
      "  Stored in directory: /home/hongky/.cache/pip/wheels/25/ed/b5/2c2be1d1a3b05855c5b9c0507b95fff40326e1a84ebc1fa6b9\n",
      "Successfully built mmcv\n",
      "Installing collected packages: addict, pyyaml, yapf, mmcv\n",
      "Successfully installed addict-2.3.0 mmcv-1.1.4 pyyaml-5.3.1 yapf-0.30.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mmcv dotmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hongky/.conda/envs/mdc/lib/python3.6/site-packages/setuptools/distutils_patch.py:26: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  \"Distutils was imported before Setuptools. This usage is discouraged \"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from models import *\n",
    "from tqdm import tqdm\n",
    "from models.slimmableops import bn_calibration_init\n",
    "import USconfig as FLAGS\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotmap import DotMap\n",
    "\n",
    "args = DotMap()\n",
    "\n",
    "args.dataset = 'imagenet'\n",
    "args.batch_size=128\n",
    "args.workers=4\n",
    "args.test_batch_size=64\n",
    "args.epochs = 1\n",
    "args.start_epoch=0\n",
    "args.lr = 0.2\n",
    "args.momentum=0.9\n",
    "args.weight_decay=1e-4\n",
    "args.resume=''\n",
    "args.no_cuda=False\n",
    "args.seed=1\n",
    "args.save='checkpoints_mobilenetv2_imagenet'\n",
    "args.arch='MobileNetV2'\n",
    "args.sr=True\n",
    "args.s=0.0001\n",
    "args.test=True\n",
    "\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "savepath = os.path.join(args.save, args.arch, 'sr' if args.sr else 'nosr')\n",
    "if not os.path.exists(savepath):\n",
    "    os.makedirs(savepath)\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "args.data = '/home/hongky/datasets/imagenet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading code\n",
    "# traindir = os.path.join(args.data, 'train')\n",
    "# valdir = os.path.join(args.data, 'val')\n",
    "# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                  std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# train_dataset = datasets.ImageFolder(\n",
    "#     traindir,\n",
    "#     transforms.Compose([\n",
    "#         transforms.RandomResizedCrop(224),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         normalize,\n",
    "#     ]))\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "#     num_workers=args.workers, pin_memory=True, sampler=None)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.ImageFolder(valdir, transforms.Compose([\n",
    "#         transforms.Resize(256),\n",
    "#         transforms.CenterCrop(224),\n",
    "#         transforms.ToTensor(),\n",
    "#         normalize,\n",
    "#     ])),\n",
    "#     batch_size=args.batch_size, shuffle=False,\n",
    "#     num_workers=args.workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Keep testset is the same for both teacher vs. student \n",
    "data_folder = args.data\n",
    "train_folder = os.path.join(data_folder, 'train')\n",
    "test_folder = os.path.join(data_folder, 'val')\n",
    "\n",
    "if not os.path.exists(test_folder):\n",
    "    raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), test_folder)\n",
    "\n",
    "if not os.path.exists(train_folder):\n",
    "    raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), train_folder)\n",
    "\n",
    "# create dataset\n",
    "train_set = datasets.ImageFolder(train_folder, transform=train_transform)\n",
    "test_set = datasets.ImageFolder(test_folder, transform=test_transform)\n",
    "\n",
    "\n",
    "#     shuffled_dataset = torch.utils.data.Subset(train_set, torch.randperm(len(train_set)).tolist())\n",
    "num_workers = args.workers\n",
    "batch_size = args.batch_size\n",
    "train_loader = DataLoader(train_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=num_workers,\n",
    "                          pin_memory=True,\n",
    "                          sampler=None)\n",
    "\n",
    "test_loader = DataLoader(test_set,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False,\n",
    "                         num_workers=num_workers,\n",
    "                         pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = eval(args.arch)(n_class=1000, input_size=224)\n",
    "print('Done eval model:', model)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "best_prec1 = -1\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "if args.resume:\n",
    "    if os.path.isfile(args.resume):\n",
    "        print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        best_prec1 = checkpoint['best_prec1']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {}) Prec1: {:f}\"\n",
    "              .format(args.resume, checkpoint['epoch'], best_prec1))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(args.resume))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def updateBN():\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.grad.data.add_(args.s * torch.sign(m.weight.data))  # L1\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    train_acc = 0.\n",
    "    \n",
    "#     batch_time = AverageMeter()\n",
    "#     losses = AverageMeter()\n",
    "#     top1 = AverageMeter()\n",
    "#     top5 = AverageMeter()\n",
    "    \n",
    "    \n",
    "    for batch_idx, (data, target) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        data = Variable(data)\n",
    "        target = Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        #F.cross_entropy(output, target)\n",
    "        #avg_loss += loss.item()\n",
    "        #pred = output.data.max(1, keepdim=True)[1]\n",
    "        #train_acc += pred.eq(target.data.view_as(pred)).sum()\n",
    "        loss.backward()\n",
    "        if args.sr:\n",
    "            updateBN()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "def trainUS():\n",
    "    max_width = max(FLAGS.width_mult_list)\n",
    "    min_width = min(FLAGS.width_mult_list)\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in tqdm(enumerate(train_loader),total=len(train_loader)):\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        ###\n",
    "        widths_train = []\n",
    "        for _ in range(getattr(FLAGS, 'num_sample_training', 2) - 2):\n",
    "            widths_train.append(\n",
    "                random.uniform(min_width, max_width))\n",
    "        widths_train = [max_width, min_width] + widths_train\n",
    "        # widths_train = [min_width]\n",
    "        for width_mult in widths_train:\n",
    "            # TODO :add inplace distillation\n",
    "            model.apply(lambda m: setattr(\n",
    "                m, 'width_mult',\n",
    "                width_mult))\n",
    "                # always track largest model and smallest model\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "        ###\n",
    "        optimizer.step()\n",
    "\n",
    "def test(epoch,test_width=1.0,recal=False):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    model.apply(lambda m: setattr(m, 'width_mult',test_width))\n",
    "    if recal:\n",
    "        model.apply(bn_calibration_init)\n",
    "        model.train()\n",
    "        for idx,(data, target) in enumerate(tqdm(train_loader, total=len(train_loader))):\n",
    "            if idx==FLAGS.recal_batch:\n",
    "                break\n",
    "            if args.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "            del output\n",
    "            \n",
    "    model.eval()\n",
    "    for data, target in tqdm(test_loader, total=len(test_loader)):\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target, size_average=False).item()  # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nEpoch: {} Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(epoch,\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return correct.item() / float(len(test_loader.dataset))\n",
    "\n",
    "def export2normal():\n",
    "    newmodel=MobileNetV2()\n",
    "    from collections import OrderedDict\n",
    "    statedic=[]\n",
    "    for k2,v in model.state_dict().items():\n",
    "        if 'running' in k2 or 'num_batches_tracked' in k2:\n",
    "            continue\n",
    "        statedic.append(v)\n",
    "    names=[]\n",
    "    for k1,v1 in newmodel.state_dict().items():\n",
    "        if 'running' in k1 or 'num_batches_tracked' in k1:\n",
    "            continue\n",
    "        names.append(k1)\n",
    "    newdic=OrderedDict(zip(names,statedic))\n",
    "    newmodel.load_state_dict(newdic,strict=False)\n",
    "    torch.save(newmodel.state_dict(),os.path.join(savepath,'trans.pth'))\n",
    "    print(\"save transferred ckpt at {}\".format(os.path.join(savepath,'trans.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prec1 = 0. if best_prec1 == -1 else best_prec1\n",
    "scheduler=optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=args.epochs,eta_min=0)\n",
    "\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "#     if args.arch=='USMobileNetV2':\n",
    "#         trainUS()\n",
    "#         prec1=test(test_width=1.0,recal=False,epoch=epoch)\n",
    "#     else:\n",
    "    train()\n",
    "    # prec1 = test(epoch=epoch)\n",
    "    scheduler.step(epoch)\n",
    "    lr_current = optimizer.param_groups[0]['lr']\n",
    "#     print(\"currnt lr:{}\".format(lr_current))\n",
    "#     is_best = prec1 > best_prec1\n",
    "#     best_prec1 = max(prec1, best_prec1)\n",
    "#     if is_best:\n",
    "#         ckptfile = os.path.join(savepath, 'model_best.pth.tar')\n",
    "#     else:\n",
    "    ckptfile = os.path.join(savepath, 'checkpoint.pth.tar')\n",
    "        \n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec1': best_prec1,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, ckptfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.arch=='USMobileNetV2':\n",
    "    export2normal()\n",
    "    res_acc=[1.0]*len(FLAGS.width_mult_list)\n",
    "    for idx,width in enumerate(FLAGS.width_mult_list):\n",
    "        acc=test(width,recal=True)\n",
    "        res_acc[idx]=acc\n",
    "        print(\"Test accuracy for width {} is {}\".format(width,acc))\n",
    "else:\n",
    "    print(\"Test accuracy {}\".format(test(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ckptfile)\n",
    "torch.save({\n",
    "    'epoch': epoch + 1,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'best_prec1': best_prec1,\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "}, ckptfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdc",
   "language": "python",
   "name": "mdc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
